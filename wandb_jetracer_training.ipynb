{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "wandb-jetracer-training",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4b245d2f0b7d4dcca82e481d9f385388": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_dc026545079c43bcb8d5610562a86657",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_2e9f5350b2d3400588e0d3e29ffbad9f",
              "IPY_MODEL_6bef52aa16d24b8dbb1e68851d5730eb"
            ]
          }
        },
        "dc026545079c43bcb8d5610562a86657": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2e9f5350b2d3400588e0d3e29ffbad9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_cb9cdc4b0c5640a5afdcf2cddf3b1709",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 60,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 60,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7cb2c8e175564779858cea5f3ae825be"
          }
        },
        "6bef52aa16d24b8dbb1e68851d5730eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_2e66a59c64874e8392b77d4febee3fad",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 60/60 [06:53&lt;00:00,  6.90s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_63fcdc2504a14600b9e3462ecc2cdc05"
          }
        },
        "cb9cdc4b0c5640a5afdcf2cddf3b1709": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7cb2c8e175564779858cea5f3ae825be": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2e66a59c64874e8392b77d4febee3fad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "63fcdc2504a14600b9e3462ecc2cdc05": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a4a369fc414d44efa7332e832151f14a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "state": {
            "_view_name": "VBoxView",
            "_dom_classes": [],
            "_model_name": "VBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_c128b0758eba4dc78cf3faf5688a1652",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_b086cff578484d978d08f042251bd8d9",
              "IPY_MODEL_69a94b6ad46b4edebca608c384c1c061"
            ]
          }
        },
        "c128b0758eba4dc78cf3faf5688a1652": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b086cff578484d978d08f042251bd8d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "state": {
            "_view_name": "LabelView",
            "style": "IPY_MODEL_61955a56b8a24ab9a18751013ec9308e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "LabelModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 42.95MB of 42.95MB uploaded (0.00MB deduped)\r",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ec707e815c6d4fb3a1fc1cef32f3c8c6"
          }
        },
        "69a94b6ad46b4edebca608c384c1c061": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_33994f9c7a1744a4839000833db20e76",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d5c4d13b4adb4c68bbf5580c1ad857aa"
          }
        },
        "61955a56b8a24ab9a18751013ec9308e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ec707e815c6d4fb3a1fc1cef32f3c8c6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "33994f9c7a1744a4839000833db20e76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d5c4d13b4adb4c68bbf5580c1ad857aa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Armandpl/wandb_jetracer/blob/master/wandb_jetracer_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEzBKTsrEUVF"
      },
      "source": [
        "<img src=\"https://i.imgur.com/gb6B4ig.png\" width=\"400\" alt=\"Weights & Biases\" />\n",
        "\n",
        "# 🔥 = W&B ➕ PyTorch ➕ Nvidia jetracer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lp5sRB1dFvfh"
      },
      "source": [
        "# 🚀 Install, Import, and Log In"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xZQizxsD-_q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "385cd19a-b2d5-40e4-9c54-dc8b696ae337"
      },
      "source": [
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Manually set pytorch seed to get the same dataset split everytime\n",
        "torch.manual_seed(42)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f27ea3e79b0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmlhYoBlOxYd"
      },
      "source": [
        "### 0️⃣ Step 0: Install W&B"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAxP8Q_HIHGy"
      },
      "source": [
        "To get started, we'll need to get the library.\n",
        "`wandb` is easily installed using `pip`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAZBTFNQq0ys"
      },
      "source": [
        "%%capture\n",
        "!pip install wandb --upgrade"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tu-KHLAeO3wi"
      },
      "source": [
        "### 1️⃣ Step 1: Import W&B and Login"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sInizMbwtnfM"
      },
      "source": [
        "In order to log data to our web service,\n",
        "you'll need to log in.\n",
        "\n",
        "If this is your first time using W&B,\n",
        "you'll need to sign up for a free account at the link that appears."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmVkQbw_q_07",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3387c990-b14a-4c64-c4ce-d83779bdacac"
      },
      "source": [
        "import wandb\n",
        "\n",
        "wandb.login()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5heF2YaeLJES"
      },
      "source": [
        "# 👩‍🔬 Define the Experiment and Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUvWDYeuJ2ZZ"
      },
      "source": [
        "## 2️⃣ Step 2: Track metadata and hyperparameters with `wandb.init`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IEUSDJByP_Yk"
      },
      "source": [
        "config = dict(\n",
        "    epochs=60,\n",
        "    architecture=\"resnet18\",\n",
        "    pretrained=True,\n",
        "    batch_size=64,\n",
        "    learning_rate=1e-4,\n",
        "    dataset=\"old-racetrack:latest\",\n",
        "    train_pct=0.8,\n",
        "    train_augs=False\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rc-v71gtuB-2"
      },
      "source": [
        "Now, let's define the overall pipeline,\n",
        "which is pretty typical for model-training:\n",
        "\n",
        "1. we first `make` a model, plus associated data and optimizer, then\n",
        "2. we `train` the model accordingly and\n",
        "3. we `optimize` the model for inference on the Jetson Nano then\n",
        "3. we `test` it to see how training went.\n",
        "4. finally we `log` both the trained model and the optimized model\n",
        "\n",
        "We'll implement these functions below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Xp0rji6BiJ7"
      },
      "source": [
        "def model_pipeline(hyperparameters):\n",
        "\n",
        "    # tell wandb to get started\n",
        "    with wandb.init(project=\"wandb-jetracer\", config=hyperparameters, job_type=\"train\") as run:\n",
        "      # access all HPs through wandb.config, so logging matches execution!\n",
        "\n",
        "      # make the model, data, and optimization problem\n",
        "      model, train_loader, test_loader, criterion, optimizer = make(run)\n",
        "      print(model)\n",
        "\n",
        "      # and use them to train the model\n",
        "      train(model, train_loader, test_loader, criterion, optimizer, run)\n",
        "\n",
        "      # once it's trained we optimize it using tensorRT\n",
        "      # trt_model = optimize(model)\n",
        "\n",
        "      # and then test its final performance\n",
        "      # test(optimized_model, test_loader, run)\n",
        "\n",
        "      # finally we log both models to wandb\n",
        "      torch.save(model.state_dict(), 'model.pth')\n",
        "      artifact = wandb.Artifact('model', type='model')\n",
        "      artifact.add_file('model.pth')\n",
        "\n",
        "      #torch.save(trt_model.state_dict(), 'trt_model.pth')\n",
        "      #trt_artifact = wandb.Artifact('trt-model', type='model')\n",
        "      #trt_artifact.add_file('trt-model.pth')\n",
        "\n",
        "      # Save the artifact version to W&B and mark it as the output of this run\n",
        "      run.log_artifact(artifact)\n",
        "      #run.log_artifact(trt_artifact)\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 649
        },
        "id": "OfHAJrfl2prf",
        "outputId": "b0ed467b-a3fd-4b8e-dfdb-27698f7b144c"
      },
      "source": [
        "!pip install -U git+https://github.com/albu/albumentations --no-cache-dir"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/albu/albumentations\n",
            "  Cloning https://github.com/albu/albumentations to /tmp/pip-req-build-qa07lava\n",
            "  Running command git clone -q https://github.com/albu/albumentations /tmp/pip-req-build-qa07lava\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from albumentations==0.5.2) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.7/dist-packages (from albumentations==0.5.2) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: scikit-image>=0.16.1 in /usr/local/lib/python3.7/dist-packages (from albumentations==0.5.2) (0.16.2)\n",
            "Requirement already satisfied, skipping upgrade: imgaug>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from albumentations==0.5.2) (0.4.0)\n",
            "Requirement already satisfied, skipping upgrade: PyYAML in /usr/local/lib/python3.7/dist-packages (from albumentations==0.5.2) (3.13)\n",
            "Requirement already satisfied, skipping upgrade: opencv-python>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from albumentations==0.5.2) (4.1.2.30)\n",
            "Requirement already satisfied, skipping upgrade: PyWavelets>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations==0.5.2) (1.1.1)\n",
            "Requirement already satisfied, skipping upgrade: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations==0.5.2) (2.5.1)\n",
            "Requirement already satisfied, skipping upgrade: pillow>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations==0.5.2) (7.1.2)\n",
            "Requirement already satisfied, skipping upgrade: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations==0.5.2) (2.4.1)\n",
            "Requirement already satisfied, skipping upgrade: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations==0.5.2) (3.2.2)\n",
            "Requirement already satisfied, skipping upgrade: Shapely in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.5.2) (1.7.1)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.5.2) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: decorator<5,>=4.3 in /usr/local/lib/python3.7/dist-packages (from networkx>=2.0->scikit-image>=0.16.1->albumentations==0.5.2) (4.4.2)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations==0.5.2) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations==0.5.2) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations==0.5.2) (1.3.1)\n",
            "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations==0.5.2) (0.10.0)\n",
            "Building wheels for collected packages: albumentations\n",
            "  Building wheel for albumentations (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for albumentations: filename=albumentations-0.5.2-cp37-none-any.whl size=88321 sha256=bf35fd9c12aa70c3060e69a1365163799bf7cdf998e92e6c1498f3ced68656d5\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-2cm1im9w/wheels/45/8b/e4/2837bbcf517d00732b8e394f8646f22b8723ac00993230188b\n",
            "Successfully built albumentations\n",
            "Installing collected packages: albumentations\n",
            "  Found existing installation: albumentations 0.5.2\n",
            "    Uninstalling albumentations-0.5.2:\n",
            "      Successfully uninstalled albumentations-0.5.2\n",
            "Successfully installed albumentations-0.5.2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "albumentations"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evk6_51zsdHN"
      },
      "source": [
        "import torch\n",
        "import os\n",
        "import glob\n",
        "import uuid\n",
        "import PIL.Image\n",
        "import torch.utils.data\n",
        "import subprocess\n",
        "import cv2\n",
        "import numpy as np\n",
        "import random\n",
        "import albumentations as A\n",
        "\n",
        "class XYDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, directory, transform=None, train=False):\n",
        "        super(XYDataset, self).__init__()\n",
        "        self.directory = directory\n",
        "        self.transform = A.Compose([\n",
        "            A.Resize(224, 224),\n",
        "            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
        "        ],\n",
        "        keypoint_params=A.KeypointParams(format='xy'),\n",
        "        )\n",
        "        self.refresh()\n",
        "        self.train = train\n",
        "\n",
        "        self.augmentations = A.Compose([\n",
        "            # A.RandomCrop(width=112, height=112, p=0.3),\n",
        "            # A.RandomBrightnessContrast(p=0.5),\n",
        "            A.ColorJitter (brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2, always_apply=False, p=0.5),\n",
        "            A.HorizontalFlip(p=0.5),\n",
        "            A.IAAPerspective (scale=(0.05, 0.1), keep_size=True, always_apply=False, p=0.3),\n",
        "            A.MotionBlur(p=0.3),\n",
        "            # A.ISONoise (color_shift=(0.01, 0.05), intensity=(0.1, 0.5), always_apply=False, p=0.3)\n",
        "            #A.OneOf([\n",
        "            #    A.HueSaturationValue(p=0.5),\n",
        "            #    A.RGBShift(p=0.7)\n",
        "            #], p=1),\n",
        "        ],\n",
        "        keypoint_params=A.KeypointParams(format='xy'),\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.annotations)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        ann = self.annotations[idx]\n",
        "        image = cv2.imread(ann['image_path'], cv2.IMREAD_COLOR)\n",
        "        image = PIL.Image.fromarray(image)\n",
        "        width = image.width\n",
        "        height = image.height\n",
        "\n",
        "        image = self.transform(image=np.array(image), keypoints=[])['image']\n",
        "\n",
        "        if self.train:\n",
        "            for i in range(10):\n",
        "                transformed = self.augmentations(image=np.array(image), keypoints=[(ann['x'], ann['y'])])\n",
        "                image = transformed['image']\n",
        "                if len(transformed['keypoints']) > 0:\n",
        "                    x, y = transformed['keypoints'][0]\n",
        "                    break\n",
        "\n",
        "        image = np.transpose(image, (2, 0, 1)).astype(np.float32)\n",
        "\n",
        "        x = 2.0 * (ann['x'] / width - 0.5) # -1 left, +1 right\n",
        "        y = 2.0 * (ann['y'] / height - 0.5) # -1 top, +1 bottom\n",
        "\n",
        "        return torch.tensor(image, dtype=torch.float), torch.Tensor([x, y])\n",
        "\n",
        "    def _parse(self, path):\n",
        "        basename = os.path.basename(path)\n",
        "        items = basename.split('_')\n",
        "        x = items[0]\n",
        "        y = items[1]\n",
        "        return int(x), int(y)\n",
        "\n",
        "    def refresh(self):\n",
        "        self.annotations = []\n",
        "        for image_path in glob.glob(os.path.join(self.directory, '*.jpg')):\n",
        "            x, y = self._parse(image_path)\n",
        "            self.annotations += [{\n",
        "                'image_path': image_path,\n",
        "                'x': x,\n",
        "                'y': y\n",
        "            }]\n",
        "        \n",
        "    def save_entry(self, image, x, y):\n",
        "        if not os.path.exists(self.directory):\n",
        "            subprocess.call(['mkdir', '-p', self.directory])\n",
        "            \n",
        "\n",
        "        height, width, _ = image.shape\n",
        "\n",
        "        x = int(x/width * 224)\n",
        "        y = int(y/height * 224)\n",
        "        filename = '%d_%d_%s.jpg' % (x, y, str(uuid.uuid1()))\n",
        "       \n",
        "        image = cv2.resize(image, (224, 224), interpolation = cv2.INTER_AREA)  \n",
        "        image_path = os.path.join(self.directory, filename)\n",
        "        cv2.imwrite(image_path, image)\n",
        "        self.refresh()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NIgZMybZMvcu"
      },
      "source": [
        "def make(run):\n",
        "    # Pull the dataset\n",
        "    artifact = run.use_artifact(run.config.dataset)\n",
        "    artifact_dir = artifact.download()\n",
        "\n",
        "    dataset_aug = XYDataset(artifact_dir, train=run.config.train_augs)\n",
        "    dataset = XYDataset(artifact_dir, train=False)\n",
        "\n",
        "    train_len = int(len(dataset)*run.config.train_pct)\n",
        "    test_len = len(dataset)-train_len\n",
        "\n",
        "    train, _ = torch.utils.data.random_split(dataset_aug, (train_len, test_len))\n",
        "    _, test = torch.utils.data.random_split(dataset, (train_len, test_len))\n",
        "\n",
        "    train_loader = make_loader(train, batch_size=run.config.batch_size)\n",
        "    test_loader = make_loader(test, batch_size=run.config.batch_size)\n",
        "\n",
        "    # Make the model\n",
        "    model = torchvision.models.__dict__[run.config.architecture](pretrained=run.config.pretrained)\n",
        "    model.fc = nn.Linear(model.fc.in_features, 2)\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Make the loss and optimizer\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = torch.optim.Adam(\n",
        "        model.parameters(), lr=run.config.learning_rate)\n",
        "    \n",
        "    return model, train_loader, test_loader, criterion, optimizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VNICjnhOT9n"
      },
      "source": [
        "def make_loader(dataset, batch_size):\n",
        "    loader = torch.utils.data.DataLoader(dataset=dataset,\n",
        "                                         batch_size=batch_size, \n",
        "                                         shuffle=True,\n",
        "                                         pin_memory=True, num_workers=8)\n",
        "    return loader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5O7stDUWJ0qm"
      },
      "source": [
        "# 👟 Define Training Logic"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZiTlrNkRKzm"
      },
      "source": [
        "def train(model, train_loader, test_loader, criterion, optimizer, run):\n",
        "    # tell wandb to watch what the model gets up to: gradients, weights, and more!\n",
        "    run.watch(model, criterion, log=\"all\", log_freq=10)\n",
        "\n",
        "    # Run training and track with wandb\n",
        "    total_batches = len(train_loader) * run.config.epochs\n",
        "    example_ct = 0  # number of examples seen\n",
        "    batch_ct = 0\n",
        "    for epoch in tqdm(range(run.config.epochs)):\n",
        "        for _, (images, labels) in enumerate(train_loader):\n",
        "\n",
        "            loss = train_batch(images, labels, model, optimizer, criterion)\n",
        "            example_ct +=  len(images)\n",
        "            batch_ct += 1\n",
        "\n",
        "            # Report metrics every batch\n",
        "            loss = float(loss)\n",
        "            wandb.log({\"epoch\": epoch, \"train_loss\": loss}, step=example_ct)\n",
        "        \n",
        "        # evaluate every epoch\n",
        "        test(model, test_loader, criterion, example_ct)\n",
        "\n",
        "def train_batch(images, labels, model, optimizer, criterion):\n",
        "    images, labels = images.to(device), labels.to(device)\n",
        "    \n",
        "    # Forward pass ➡\n",
        "    outputs = model(images)\n",
        "    loss = criterion(outputs, labels)\n",
        "    \n",
        "    # Backward pass ⬅\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "\n",
        "    # Step with optimizer\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-dy-1i3IZZc"
      },
      "source": [
        "# 🧪 Define Testing Logic"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKitRLZsYzCP"
      },
      "source": [
        "def test(model, test_loader, criterion, example_ct):\n",
        "    model.eval()\n",
        "\n",
        "    # Run the model on some test examples\n",
        "    with torch.no_grad():\n",
        "\n",
        "        mean_loss = None\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            if mean_loss is None:\n",
        "              mean_loss = loss\n",
        "            else:\n",
        "              mean_loss = (loss+mean_loss)/2\n",
        "\n",
        "        mean_loss = float(mean_loss)\n",
        "        print(\"{} test loss\".format(mean_loss))\n",
        "        \n",
        "        wandb.log({\"test_loss\": mean_loss}, step=example_ct)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2koDFXb4DCzH"
      },
      "source": [
        "# 🏃‍♀️ Run training and watch your metrics live on [wandb.ai](https://wandb.ai)!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPQG1_I8DB7U",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "4b245d2f0b7d4dcca82e481d9f385388",
            "dc026545079c43bcb8d5610562a86657",
            "2e9f5350b2d3400588e0d3e29ffbad9f",
            "6bef52aa16d24b8dbb1e68851d5730eb",
            "cb9cdc4b0c5640a5afdcf2cddf3b1709",
            "7cb2c8e175564779858cea5f3ae825be",
            "2e66a59c64874e8392b77d4febee3fad",
            "63fcdc2504a14600b9e3462ecc2cdc05",
            "a4a369fc414d44efa7332e832151f14a",
            "c128b0758eba4dc78cf3faf5688a1652",
            "b086cff578484d978d08f042251bd8d9",
            "69a94b6ad46b4edebca608c384c1c061",
            "61955a56b8a24ab9a18751013ec9308e",
            "ec707e815c6d4fb3a1fc1cef32f3c8c6",
            "33994f9c7a1744a4839000833db20e76",
            "d5c4d13b4adb4c68bbf5580c1ad857aa"
          ]
        },
        "outputId": "f4514c49-c413-499e-a463-4bf2bce0a1c1"
      },
      "source": [
        "# Build, train and analyze the model with the pipeline\n",
        "model = model_pipeline(config)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.10.27<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">rural-leaf-108</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/armandpl2/wandb-jetracer\" target=\"_blank\">https://wandb.ai/armandpl2/wandb-jetracer</a><br/>\n",
              "                Run page: <a href=\"https://wandb.ai/armandpl2/wandb-jetracer/runs/3j17cxld\" target=\"_blank\">https://wandb.ai/armandpl2/wandb-jetracer/runs/3j17cxld</a><br/>\n",
              "                Run data is saved locally in <code>/content/wandb/run-20210428_152919-3j17cxld</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/albumentations/imgaug/transforms.py:334: FutureWarning: IAAPerspective is deprecated. Please use Perspective instead\n",
            "  warnings.warn(\"IAAPerspective is deprecated. Please use Perspective instead\", FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4b245d2f0b7d4dcca82e481d9f385388",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=60.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:795: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.34096258878707886 test loss\n",
            "0.11732679605484009 test loss\n",
            "0.06136243790388107 test loss\n",
            "0.027065493166446686 test loss\n",
            "0.019289672374725342 test loss\n",
            "0.013117067515850067 test loss\n",
            "0.0119261359795928 test loss\n",
            "0.008784420788288116 test loss\n",
            "0.008017823100090027 test loss\n",
            "0.006177249364554882 test loss\n",
            "0.00558243365958333 test loss\n",
            "0.006223681848496199 test loss\n",
            "0.00597898755222559 test loss\n",
            "0.004403924569487572 test loss\n",
            "0.003941160626709461 test loss\n",
            "0.00601920485496521 test loss\n",
            "0.005033294670283794 test loss\n",
            "0.005099016707390547 test loss\n",
            "0.004731135442852974 test loss\n",
            "0.004077061545103788 test loss\n",
            "0.003266919869929552 test loss\n",
            "0.004597066901624203 test loss\n",
            "0.00433465838432312 test loss\n",
            "0.003947819583117962 test loss\n",
            "0.0055326796136796474 test loss\n",
            "0.003501846920698881 test loss\n",
            "0.0029077078215777874 test loss\n",
            "0.0030030477792024612 test loss\n",
            "0.0035431724973022938 test loss\n",
            "0.00375111261382699 test loss\n",
            "0.003124529030174017 test loss\n",
            "0.003972472622990608 test loss\n",
            "0.0033971865195780993 test loss\n",
            "0.003594270208850503 test loss\n",
            "0.0060028573498129845 test loss\n",
            "0.004571050405502319 test loss\n",
            "0.003337541129440069 test loss\n",
            "0.003290839260444045 test loss\n",
            "0.0037272118497639894 test loss\n",
            "0.003142336616292596 test loss\n",
            "0.003795469179749489 test loss\n",
            "0.005153900012373924 test loss\n",
            "0.003295176662504673 test loss\n",
            "0.003497322555631399 test loss\n",
            "0.003451970871537924 test loss\n",
            "0.0033133788965642452 test loss\n",
            "0.0031867241486907005 test loss\n",
            "0.003628902602940798 test loss\n",
            "0.0035225804895162582 test loss\n",
            "0.004060892388224602 test loss\n",
            "0.00318971648812294 test loss\n",
            "0.004047092515975237 test loss\n",
            "0.003268986940383911 test loss\n",
            "0.0033998549915850163 test loss\n",
            "0.005384714342653751 test loss\n",
            "0.0039588287472724915 test loss\n",
            "0.004152591340243816 test loss\n",
            "0.004427359439432621 test loss\n",
            "0.004141079727560282 test loss\n",
            "0.004057468380779028 test loss\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<br/>Waiting for W&B process to finish, PID 22262<br/>Program ended successfully."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a4a369fc414d44efa7332e832151f14a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "VBox(children=(Label(value=' 42.72MB of 42.72MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Find user logs for this run at: <code>/content/wandb/run-20210428_152919-3j17cxld/logs/debug.log</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Find internal logs for this run at: <code>/content/wandb/run-20210428_152919-3j17cxld/logs/debug-internal.log</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<h3>Run summary:</h3><br/><style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    </style><table class=\"wandb\">\n",
              "<tr><td>epoch</td><td>59</td></tr><tr><td>train_loss</td><td>0.0025</td></tr><tr><td>_runtime</td><td>420</td></tr><tr><td>_timestamp</td><td>1619624179</td></tr><tr><td>_step</td><td>56220</td></tr><tr><td>test_loss</td><td>0.00406</td></tr></table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<h3>Run history:</h3><br/><style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    </style><table class=\"wandb\">\n",
              "<tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train_loss</td><td>█▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>_runtime</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_timestamp</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>test_loss</td><td>█▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Synced 5 W&B file(s), 1 media file(s), 1 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                    <br/>Synced <strong style=\"color:#cdcd00\">rural-leaf-108</strong>: <a href=\"https://wandb.ai/armandpl2/wandb-jetracer/runs/3j17cxld\" target=\"_blank\">https://wandb.ai/armandpl2/wandb-jetracer/runs/3j17cxld</a><br/>\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}