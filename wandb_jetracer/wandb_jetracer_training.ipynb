{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "wandb-jetracer-training",
      "provenance": [],
      "collapsed_sections": [
        "nLkynvW3wffG"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Armandpl/wandb-jetracer/blob/master/wandb_jetracer/wandb_jetracer_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEzBKTsrEUVF"
      },
      "source": [
        "<img src=\"https://i.imgur.com/gb6B4ig.png\" width=\"400\" alt=\"Weights & Biases\" />\n",
        "\n",
        "# üèÅüèéÔ∏èüí® = W&B ‚ûï NVIDIA Jetracer\n",
        "\n",
        "This is a companion notebook for [title of the rc car video](). We are going to use [PyTorch Lightning](https://www.pytorchlightning.ai/) and [Weights&Biases](https://wandb.ai/site) to train a neural net that can drive [an NVIDIA Jetracer RC car](https://github.com/NVIDIA-AI-IOT/jetracer). \n",
        "\n",
        "Weights&Biases is a lightweight developper toolkit for [experiment tracking](https://wandb.ai/site/experiment-tracking), [model management and dataset versioning](https://wandb.ai/site/artifacts). Going through this notebook you'll see how we're leveraging it to effortlessly train and deploy models to the RC car while having good tracability of what model was trained on which version of the dataset with which hyperparameters.\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://raw.githubusercontent.com/Armandpl/wandb-jetracer/master/assets/header.png\">\n",
        "</p>\n",
        "<p align=\"center\">\n",
        "<i>Here is the car's pov along with it's label (green circle) indicating the center of the 'road'.</i>\n",
        "</p>\n",
        "\n",
        "You can experiment with this notebook even if you don't have access to an RC car. If you run an interesting experiment ping me [@armand_dpl](https://twitter.com/armand_dpl) and I will try it on the car!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLkynvW3wffG"
      },
      "source": [
        "# 0. Setup\n",
        "Here we are installing and importing dependencies.  \n",
        "We are also cloning https://github.com/Armandpl/wandb_jetracer to get util functions. This repository builds on top of the [NVIDIA Jetracer](https://github.com/NVIDIA-AI-IOT/jetracer) project to instrument it with Weights&Biases. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "plsVfM5EXPgJ"
      },
      "source": [
        "!pip install wandb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOlD-e8_ELdZ"
      },
      "source": [
        "!git clone https://github.com/Armandpl/wandb_jetracer\n",
        "!pip install pytorch-lightning torchmetrics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xZQizxsD-_q"
      },
      "source": [
        "import math\n",
        "import os\n",
        "\n",
        "import cv2\n",
        "import PIL\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.loggers import WandbLogger\n",
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchmetrics\n",
        "import wandb\n",
        "\n",
        "from wandb_jetracer.utils.xy_dataset import XYDataset\n",
        "from wandb_jetracer.utils.utils import show_label, torch2cv2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zuJA1mibLg5H"
      },
      "source": [
        "# 1. Training a model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQLch08VhRws"
      },
      "source": [
        "The idea to get the car to drive by itself is to train a model to infer the center of the road. If the center is on the left we steer to the left, if it's on the right we steer to the right.  \n",
        "\n",
        "To solve this regression task and predict the center of the racetrack we fine-tune a [ResNet](https://arxiv.org/abs/1512.03385) model. We replace the last fully-connected layer with our own fully-connected layer.  \n",
        "\n",
        "If you want to experiment with [the model architecture](https://pytorch.org/vision/stable/models.html), feel free to modify the `build_model` function. Maybe you could try a MobileNet architecture? or an EfficientNet? or a simple convolutional network?  \n",
        "\n",
        "If you tried a different architecture and are proud of your results feel free to send me a DM [@armand_dpl](https://twitter.com/armand_dpl) and I will try to run your model on the actual car!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0TcMkrURwyUQ"
      },
      "source": [
        "def build_model():\n",
        "    model = torchvision.models.__dict__[config.architecture](pretrained=config.pretrained)\n",
        "    model.fc = nn.Linear(model.fc.in_features, 2)\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUqI154NnaMn"
      },
      "source": [
        "Most of the code in the cell below is pretty standard. If you'd like to learn more about Weights and Biases and [PyTorch Lightning](https://www.pytorchlightning.ai/) you can check out this video: [‚ö° Supercharge your Training with PyTorch Lightning + Weights & Biases](https://www.youtube.com/watch?v=hUXQm46TAKc)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3aT-NcODcGC"
      },
      "source": [
        "class RoadRegression(pl.LightningModule):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        # setting up metrics\n",
        "        metrics = torchmetrics.MetricCollection([\n",
        "            torchmetrics.MeanSquaredError(),\n",
        "            torchmetrics.MeanAbsoluteError()\n",
        "        ])\n",
        "        self.train_metrics = metrics.clone(prefix='train/')\n",
        "        self.valid_metrics = metrics.clone(prefix='val/')\n",
        "        self.test_metrics = metrics.clone(prefix='test/')\n",
        "\n",
        "        self.model = build_model()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        images, targets = batch\n",
        "        preds = self.forward(images)\n",
        "        loss = getattr(F, self.config.loss)(preds, targets)\n",
        "\n",
        "        metrics = self.train_metrics(preds, targets)\n",
        "        self.log_dict(metrics, on_step=True, on_epoch=False)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        images, targets = batch\n",
        "        preds = self.forward(images)\n",
        "\n",
        "        metrics = self.valid_metrics(preds, targets)\n",
        "        self.log_dict(metrics, on_step=False, on_epoch=True)\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        images, targets = batch\n",
        "        preds = self.forward(images)\n",
        "\n",
        "        metrics = self.test_metrics(preds, targets)\n",
        "        self.log_dict(metrics, on_step=False, on_epoch=True)\n",
        "\n",
        "        return (images, preds, targets)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=self.config.learning_rate)\n",
        "        return optimizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjx3Co4RJecz"
      },
      "source": [
        "## Visualizing predictions on the test set  \n",
        "To get an intuition of how our model is doing we log our test set along with predictions, losses and ground truth as a [wandb table](https://docs.wandb.ai/guides/data-vis). We can then go to our dashboard and explore the predictions. For example we can sort by highest losses to gauge what's difficult for the model.  \n",
        "If you want to see what that looks like you can checkout this [run's page](https://wandb.ai/wandb/racecar/runs/16dlsdf7)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hqDisn_Kdig"
      },
      "source": [
        "def test_epoch_end(self, test_step_outputs):\n",
        "    images, predictions, targets = self.concat_test_outputs(test_step_outputs)\n",
        "\n",
        "    # compute loss for each image of the test set \n",
        "    losses = F.mse_loss(predictions, targets, reduction='none')\n",
        "\n",
        "    test_table = self.create_table(images, predictions, targets, losses)\n",
        "\n",
        "    wandb.log({\"test/predictions\": test_table})\n",
        "\n",
        "def create_table(self, images, predictions, targets, losses):\n",
        "    # display preds and targets on images\n",
        "    images_with_preds = []\n",
        "    for idx, image in enumerate(images):\n",
        "        img = torch2cv2(image)\n",
        "\n",
        "        # show ground truth and prediction on the image\n",
        "        img = show_label(img, targets[idx])\n",
        "        img = show_label(img, predictions[idx], (0, 0, 255))\n",
        "\n",
        "        images_with_preds.append(img)\n",
        "\n",
        "    # create a WandB table\n",
        "    my_data = [\n",
        "        [wandb.Image(img), pred, target, loss.sum()] \n",
        "        for img, pred, target, loss\n",
        "        in zip(images_with_preds, predictions, targets, losses)\n",
        "    ]\n",
        "\n",
        "    columns= [\"image\", \"prediction\", \"target\", \"loss\"]\n",
        "    table = wandb.Table(data=my_data, columns=columns)\n",
        "\n",
        "    return table\n",
        "\n",
        "def concat_test_outputs(self, test_step_outputs):\n",
        "    \"\"\"\n",
        "    Concatenate the output of the test step so that we can easily iterate on it and\n",
        "    compute the loss for each item in one go.\n",
        "    \"\"\"\n",
        "    images, predictions, targets = test_step_outputs[0]\n",
        "    for i in range(1, len(test_step_outputs)):\n",
        "        imgs, preds, targs = test_step_outputs[i]\n",
        "\n",
        "        images = torch.cat((images, imgs), dim=0)\n",
        "        predictions = torch.cat((predictions, preds), dim=0)\n",
        "        targets = torch.cat((targets, targs), dim=0)\n",
        "    \n",
        "    return images, predictions, targets\n",
        "\n",
        "RoadRegression.test_epoch_end = test_epoch_end\n",
        "RoadRegression.create_table = create_table\n",
        "RoadRegression.concat_test_outputs = concat_test_outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCYEoFz7K-Lv"
      },
      "source": [
        "## Preparing our data\n",
        "We download our pre-processed dataset from [WandB Artifacts](https://docs.wandb.ai/guides/artifacts). \n",
        "This way we will always know which model was trained on which version of the data, how the training went and we will be able to retrieve the trained weights."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "at-zcVInLSR6"
      },
      "source": [
        "def prepare_data(self):\n",
        "    artifact = wandb.use_artifact(self.dataset_artifact)\n",
        "    self.artifact_dir = artifact.download()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsByIVRW14ch"
      },
      "source": [
        "<p align=\"center\">\n",
        "  <img src=\"https://raw.githubusercontent.com/Armandpl/wandb-jetracer/master/assets/artifacts.png\" height=\"300\">\n",
        "</p>\n",
        "<p align=\"center\">\n",
        "<i>WandB Artifacts' graph for this project</i>\n",
        "</p>\n",
        "\n",
        "When using artifacts WandB automatically generates a graph allowing us to visualize the whole pipeline.  \n",
        "Squares represent runs, circles represent artifacts and arrows indicate what artifacts are produced/consumed by runs. For example the `train`ing runs consume a `dataset` artifact and output a `model` artifact.  \n",
        "Each of these artifact is also version controlled and we can easily access older models/datasets versions along with the runs that created them. \n",
        "[Click here](https://wandb.ai/wandb/racecar/artifacts/model/trt-model/d6bca0257e1bcec39983/graph) if you wish to explore this interactive graph!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0t8UGisroRAM"
      },
      "source": [
        "In the cell below we setup a [PyTorch Lightning Data Module](https://pytorch-lightning.readthedocs.io/en/stable/extensions/datamodules.html). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TzZ_xNCh7VCe"
      },
      "source": [
        "from typing import Optional\n",
        "\n",
        "class RoadDataModule(pl.LightningDataModule):\n",
        "\n",
        "    def __init__(self, dataset_artifact: str, batch_size):\n",
        "        super().__init__()\n",
        "        self.dataset_artifact = dataset_artifact\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def setup(self, stage: Optional[str] = None):\n",
        "        # Assign train/val datasets for use in dataloaders\n",
        "        train_pth, val_pth, test_pth = [os.path.join(self.artifact_dir, split) for split in [\"train\", \"val\", \"test\"]] \n",
        "\n",
        "        if stage == 'fit' or stage is None:\n",
        "            self.train, self.val = XYDataset(train_pth, train=True), XYDataset(val_pth, train=False)\n",
        "\n",
        "            self.dims = tuple(self.train[0][0].size())\n",
        "\n",
        "        # Assign test dataset for use in dataloader(s)\n",
        "        if stage == 'test' or stage is None:\n",
        "            self.test = XYDataset(test_pth, train=False)\n",
        "\n",
        "            self.dims = tuple(self.test[0][0].size())\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return self.make_loader(self.train, True)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return self.make_loader(self.val, False)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return self.make_loader(self.test, False)\n",
        "\n",
        "    def make_loader(self, dataset, shuffle):\n",
        "        return DataLoader(dataset=dataset,\n",
        "                          batch_size=self.batch_size, \n",
        "                          shuffle=shuffle,\n",
        "                          pin_memory=True, num_workers=2) \n",
        "\n",
        "RoadDataModule.prepare_data = prepare_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlEq-Jh_L2rL"
      },
      "source": [
        "### Let's train!\n",
        "This is also pretty standard PyTorch Lighting/WandB code. More about PyTorch Lightning and WandB [in our docs](https://docs.wandb.ai/guides/integrations/lightning)!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQMnzUBdsnnm"
      },
      "source": [
        "config = dict(\n",
        "    epochs=10,\n",
        "    architecture=\"resnet34\",\n",
        "    pretrained=True,\n",
        "    batch_size=64,\n",
        "    learning_rate=1e-4,\n",
        "    dataset=\"mix_ready:latest\",\n",
        "    train_augs=False,\n",
        "    loss=\"mse_loss\"\n",
        "    )\n",
        "\n",
        "with wandb.init(project=\"racecar\", config=config, job_type=\"train\", entity=\"wandb\") as run:\n",
        "    config = run.config\n",
        "\n",
        "    dm = RoadDataModule(config.dataset, config.batch_size)\n",
        "    road_regression = RoadRegression(config)\n",
        "\n",
        "    wandb_logger = WandbLogger()\n",
        "    trainer = pl.Trainer(\n",
        "        logger=wandb_logger,\n",
        "        gpus=1,\n",
        "        max_epochs=config.epochs,\n",
        "        log_every_n_steps=1\n",
        "    )\n",
        "    trainer.fit(road_regression, dm)\n",
        "\n",
        "    trainer.test()\n",
        "\n",
        "    # finally we log the model to wandb.\n",
        "    torch.save(road_regression.model.state_dict(), \"model.pth\")\n",
        "    artifact = wandb.Artifact('model', type='model')\n",
        "    artifact.add_file('model.pth')\n",
        "    run.log_artifact(artifact)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzdUHFSWDE8v"
      },
      "source": [
        "# 2. Working out your own dataset split\n",
        "\n",
        "Here I have mixed images from three different miniature racetracks I built: [Suzuka](https://wandb.ai/wandb/racecar/artifacts/dataset/suzuka), [Monza](https://wandb.ai/wandb/racecar/artifacts/dataset/monza) and [Nurburgring](https://wandb.ai/wandb/racecar/artifacts/dataset/nurburgring).  \n",
        "\n",
        "You may wish to tailor your own split to experiment.  \n",
        "Maybe you could bring in [images from real roads](https://github.com/commaai/comma2k19)? Maybe you could train a model on one track and evaluate it on another to see if it generalizes well?  \n",
        "Feel free to modify the code below to achieve what you want. Make sure the artifact/folder your upload to wandb after this step contains a `train`, `val` and `test` folder so that it works with the training code above!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pv-GFGCqGdET"
      },
      "source": [
        "import random\n",
        "import shutil\n",
        "\n",
        "from wandb_jetracer.utils.utils import make_dirs, split_list_by_pct\n",
        "\n",
        "config = dict(\n",
        "    datasets=[\"suzuka:latest\", \"monza:latest\", \"nurburgring:latest\"],\n",
        "    output_dataset=\"mix_ready\",\n",
        "    split_pcts=[0.7, 0.2, 0.1],\n",
        ")\n",
        "\n",
        "with wandb.init(project=\"racecar\", config=config, entity=\"wandb\", job_type=\"pre-process-dataset\") as run:\n",
        "    config = run.config\n",
        "\n",
        "    out_dirs = make_dirs(\"./tmp/\")\n",
        "\n",
        "    # make sure the train/val/test pct are coherent\n",
        "    assert math.fsum(config.split_pcts) == 1\n",
        "\n",
        "    for dataset in config.datasets:\n",
        "        # download each dataset\n",
        "        artifact = run.use_artifact(dataset)\n",
        "        artifact_dir = artifact.download()\n",
        "\n",
        "        all_fnames = os.listdir(artifact_dir)\n",
        "        random.shuffle(all_fnames)\n",
        "\n",
        "        train, val, test = split_list_by_pct(all_fnames, config.split_pcts)\n",
        "        sets = {\n",
        "            \"train\": train,\n",
        "            \"val\": val,\n",
        "            \"test\": test\n",
        "        }\n",
        "\n",
        "        # and move it's files to the train/val/test dirs for the 'mixed' dataset \n",
        "        for out_dir, split in zip(out_dirs, [\"train\", \"val\", \"test\"]):\n",
        "            for fname in sets[split]:\n",
        "                source = os.path.join(artifact_dir, fname)\n",
        "                dest = os.path.join(out_dir, fname)\n",
        "                os.rename(source, dest)\n",
        "    \n",
        "    # then upload the mixed dataset to wandb\n",
        "    artifact = wandb.Artifact(config.output_dataset, type='dataset')\n",
        "    artifact.add_dir(\"./tmp/\")\n",
        "    run.log_artifact(artifact)\n",
        "\n",
        "    shutil.rmtree('./tmp/', ignore_errors=True)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}